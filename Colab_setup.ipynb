{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvxJOJu4XUIW"
      },
      "source": [
        "### Step 1: Mount the Google Drive\n",
        "\n",
        "Remember to use GPU runtime before mounting your Google Drive. (Runtime --> Change runtime type)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCSU4HrvkVDq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyoSL1U8Xbjh"
      },
      "source": [
        "### Step 2: Open the project directory\n",
        "\n",
        "Replace `Your_Dir` with your own path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gfQ17SmkfOK"
      },
      "outputs": [],
      "source": [
        "cd Your_Dir/emg2qwerty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTzYfAOEYN4C"
      },
      "source": [
        "### Step 3: Install required packages\n",
        "\n",
        "After installing them, Colab will require you to restart the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFFKvhs4tAp5"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSoRzGXCfUtz"
      },
      "source": [
        "### Step 4: Start your experiments!\n",
        "\n",
        "- Remember to download and copy the dataset to this directory: `Your_Dir/emg2qwerty/data`.\n",
        "- You may now start your experiments with any scripts! Below are examples of single-user training and testing (greedy decoding).\n",
        "- **There are two ways to track the logs:**\n",
        "  - 1. Keep `--multirun`, and the logs will not be printed here, but they will be saved in the folder `logs`, e.g., `logs/2025-02-09/18-24-15/submitit_logs/`.\n",
        "  - 2. Comment out `--multirun` and the logs will be printed in this notebook, but they will not be saved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVuSn4rXhLJa"
      },
      "source": [
        "#### Training\n",
        "\n",
        "- The checkpoints are saved in the folder `logs`, e.g., `logs/2025-02-09/18-24-15/checkpoints/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n84M6KLmkp2i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-03-08 02:22:21,223][HYDRA] Submitit 'local' sweep output dir : logs/2025-03-08/02-22-20\n",
            "[2025-03-08 02:22:21,226][HYDRA] \t#0 : user=single_user trainer.accelerator=gpu trainer.devices=1 trainer.max_epochs=40 batch_size=8\n",
            "Error executing job with overrides: ['user=single_user', 'trainer.accelerator=gpu', 'trainer.devices=1', 'trainer.max_epochs=40', 'batch_size=8']\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/workspace/emg2qwerty/train.py\", line 129, in <module>\n",
            "    main()\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/hydra/main.py\", line 94, in decorated_main\n",
            "    _run_hydra(\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n",
            "    _run_app(\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 465, in _run_app\n",
            "    run_and_report(\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 223, in run_and_report\n",
            "    raise ex\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n",
            "    return func()\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 466, in <lambda>\n",
            "    lambda: hydra.multirun(\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/hydra/_internal/hydra.py\", line 162, in multirun\n",
            "    ret = sweeper.sweep(arguments=task_overrides)\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/hydra/_internal/core_plugins/basic_sweeper.py\", line 181, in sweep\n",
            "    _ = r.return_value\n",
            "  File \"/home/user/.local/lib/python3.10/site-packages/hydra/core/utils.py\", line 260, in return_value\n",
            "    raise self._return_value\n",
            "RuntimeError: Caught RuntimeError in pin memory thread for device 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 37, in do_one_step\n",
            "    data = pin_memory(data, device)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 68, in pin_memory\n",
            "    clone.update({k: pin_memory(sample, device) for k, sample in data.items()})\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 68, in <dictcomp>\n",
            "    clone.update({k: pin_memory(sample, device) for k, sample in data.items()})\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 58, in pin_memory\n",
            "    return data.pin_memory(device)\n",
            "RuntimeError: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Please note the following:\n",
        "#  - I've added PYTHONPATH because my environment has some bugs --> please remove it before running\n",
        "#  - I run with batch_size=8 to avoid CUDA OOM failures\n",
        "#  - CUDA still throws an OOM at test-time, even if training succeeded\n",
        "\n",
        "# Single-user training\n",
        "!PYTORCH_ENABLE_MPS_FALLBACK=1 \\\n",
        "  HYDRA_FULL_ERROR=1 \\\n",
        "  PYTHONPATH=/opt/conda/lib/python3.10/site-packages \\\n",
        "  python -m emg2qwerty.train \\\n",
        "  user=\"single_user\" \\\n",
        "  trainer.accelerator=gpu \\\n",
        "  trainer.devices=1 \\\n",
        "  trainer.max_epochs=40 \\\n",
        "  batch_size=8 \\\n",
        "  --multirun"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGANotiwhngl"
      },
      "source": [
        "#### Testing:\n",
        "\n",
        "- Replace `Your_Path_to_Checkpoint` with your checkpoint path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "p68aDt-8pmGj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-03-08 05:26:42,410][__main__][INFO] - \n",
            "Config:\n",
            "user: single_user\n",
            "dataset:\n",
            "  train:\n",
            "  - user: 89335547\n",
            "    session: 2021-06-03-1622765527-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-06-02-1622681518-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-06-04-1622863166-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-07-22-1627003020-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-07-21-1626916256-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-07-22-1627004019-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-06-05-1622885888-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-06-02-1622679967-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-06-03-1622764398-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-07-21-1626917264-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-06-05-1622889105-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-06-03-1622766673-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-06-04-1622861066-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-07-22-1627001995-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-06-05-1622884635-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  - user: 89335547\n",
            "    session: 2021-07-21-1626915176-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  val:\n",
            "  - user: 89335547\n",
            "    session: 2021-06-04-1622862148-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  test:\n",
            "  - user: 89335547\n",
            "    session: 2021-06-02-1622682789-keystrokes-dca-study@1-0efbe614-9ae6-4131-9192-4398359b4f5f\n",
            "  root: ${hydra:runtime.cwd}/data\n",
            "to_tensor:\n",
            "  _target_: emg2qwerty.transforms.ToTensor\n",
            "  fields:\n",
            "  - emg_left\n",
            "  - emg_right\n",
            "band_rotation:\n",
            "  _target_: emg2qwerty.transforms.ForEach\n",
            "  transform:\n",
            "    _target_: emg2qwerty.transforms.RandomBandRotation\n",
            "    offsets:\n",
            "    - -1\n",
            "    - 0\n",
            "    - 1\n",
            "temporal_jitter:\n",
            "  _target_: emg2qwerty.transforms.TemporalAlignmentJitter\n",
            "  max_offset: 120\n",
            "logspec:\n",
            "  _target_: emg2qwerty.transforms.LogSpectrogram\n",
            "  n_fft: 64\n",
            "  hop_length: 16\n",
            "specaug:\n",
            "  _target_: emg2qwerty.transforms.SpecAugment\n",
            "  n_time_masks: 3\n",
            "  time_mask_param: 25\n",
            "  n_freq_masks: 2\n",
            "  freq_mask_param: 4\n",
            "transforms:\n",
            "  train:\n",
            "  - ${to_tensor}\n",
            "  - ${band_rotation}\n",
            "  - ${temporal_jitter}\n",
            "  - ${logspec}\n",
            "  - ${specaug}\n",
            "  val:\n",
            "  - ${to_tensor}\n",
            "  - ${logspec}\n",
            "  test: ${transforms.val}\n",
            "module:\n",
            "  _target_: emg2qwerty.lightning.TDSConvCTCModule\n",
            "  in_features: 528\n",
            "  mlp_features:\n",
            "  - 384\n",
            "  block_channels:\n",
            "  - 24\n",
            "  - 24\n",
            "  - 24\n",
            "  - 24\n",
            "  kernel_width: 32\n",
            "datamodule:\n",
            "  _target_: emg2qwerty.lightning.WindowedEMGDataModule\n",
            "  window_length: 8000\n",
            "  padding:\n",
            "  - 1800\n",
            "  - 200\n",
            "optimizer:\n",
            "  _target_: torch.optim.Adam\n",
            "  lr: 0.001\n",
            "lr_scheduler:\n",
            "  scheduler:\n",
            "    _target_: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR\n",
            "    warmup_epochs: 10\n",
            "    max_epochs: ${trainer.max_epochs}\n",
            "    warmup_start_lr: 1.0e-08\n",
            "    eta_min: 1.0e-06\n",
            "  interval: epoch\n",
            "decoder:\n",
            "  _target_: emg2qwerty.decoder.CTCGreedyDecoder\n",
            "seed: 1501\n",
            "batch_size: 32\n",
            "num_workers: 4\n",
            "train: false\n",
            "checkpoint: /workspace/logs/2025-03-08/02-22-20/job0_batch_size=8,trainer.devices=1,trainer.max_epochs=40,user=single_user/checkpoints/epoch=37-step=18240.ckpt\n",
            "monitor_metric: val/CER\n",
            "monitor_mode: min\n",
            "trainer:\n",
            "  accelerator: cpu\n",
            "  devices: 1\n",
            "  num_nodes: 1\n",
            "  max_epochs: 150\n",
            "  default_root_dir: ${hydra:runtime.output_dir}\n",
            "callbacks:\n",
            "- _target_: pytorch_lightning.callbacks.LearningRateMonitor\n",
            "- _target_: pytorch_lightning.callbacks.ModelCheckpoint\n",
            "  dirpath: ${hydra:runtime.output_dir}/checkpoints\n",
            "  monitor: ${monitor_metric}\n",
            "  mode: ${monitor_mode}\n",
            "  save_last: true\n",
            "  verbose: true\n",
            "\n",
            "Global seed set to 1501\n",
            "[2025-03-08 05:26:42,418][__main__][INFO] - Instantiating LightningModule {'_target_': 'emg2qwerty.lightning.TDSConvCTCModule', 'in_features': 528, 'mlp_features': [384], 'block_channels': [24, 24, 24, 24], 'kernel_width': 32}\n",
            "[2025-03-08 05:26:42,600][__main__][INFO] - Loading module from checkpoint /workspace/logs/2025-03-08/02-22-20/job0_batch_size=8,trainer.devices=1,trainer.max_epochs=40,user=single_user/checkpoints/epoch=37-step=18240.ckpt\n",
            "[2025-03-08 05:26:42,897][__main__][INFO] - Instantiating LightningDataModule {'_target_': 'emg2qwerty.lightning.WindowedEMGDataModule', 'window_length': 8000, 'padding': [1800, 200]}\n",
            "GPU available: True (cuda), used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/home/user/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
            "  rank_zero_warn(\n",
            "Missing logger folder: /workspace/logs/2025-03-08/05-26-42/lightning_logs\n",
            "Validation DataLoader 0: 100%|████████████████████| 7/7 [00:10<00:00,  1.49s/it]\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "         val/CER            19.273372650146484\n",
            "         val/DER            1.6836508512496948\n",
            "         val/IER            3.8989808559417725\n",
            "         val/SER            13.690739631652832\n",
            "        val/loss            0.7159506678581238\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Testing DataLoader 0: 100%|███████████████████████| 1/1 [00:40<00:00, 40.25s/it]\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "        test/CER            20.726173400878906\n",
            "        test/DER            1.9451048374176025\n",
            "        test/IER            3.4147396087646484\n",
            "        test/SER            15.366328239440918\n",
            "        test/loss           0.7403390407562256\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "{'val_metrics': [{'val/loss': 0.7159506678581238,\n",
            "                  'val/CER': 19.273372650146484,\n",
            "                  'val/IER': 3.8989808559417725,\n",
            "                  'val/DER': 1.6836508512496948,\n",
            "                  'val/SER': 13.690739631652832}],\n",
            " 'test_metrics': [{'test/loss': 0.7403390407562256,\n",
            "                   'test/CER': 20.726173400878906,\n",
            "                   'test/IER': 3.4147396087646484,\n",
            "                   'test/DER': 1.9451048374176025,\n",
            "                   'test/SER': 15.366328239440918}],\n",
            " 'best_checkpoint': ''}\n"
          ]
        }
      ],
      "source": [
        "# Single-user testing\n",
        "!PYTORCH_ENABLE_MPS_FALLBACK=1 \\\n",
        "  HYDRA_FULL_ERROR=1 \\\n",
        "  CUDA_LAUNCH_BLOCKING=1 \\\n",
        "  python -m emg2qwerty.train \\\n",
        "  user=\"single_user\" \\\n",
        "  checkpoint=\"'/workspace/logs/2025-03-08/02-22-20/job0_batch_size=8,trainer.devices=1,trainer.max_epochs=40,user=single_user/checkpoints/epoch=37-step=18240.ckpt'\" \\\n",
        "  train=False trainer.accelerator=cpu \\\n",
        "  decoder=ctc_greedy \\\n",
        "  hydra.launcher.mem_gb=64 \\\n",
        "  # --multirun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!tensorboard --logdir='/Users/jonathonyoungquist/Local/ECE_C147/Project/emg2qwerty/logs/2025-03-01/11-14-51/job0_trainer.devices=1,trainer.max_epochs=40,user=single_user/lightning_logs/version_0/'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
